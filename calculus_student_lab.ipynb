{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calculus & Gradients â€” FAANG-Level Lab\n",
        "\n",
        "**Goal:** Implement and verify gradients like an ML engineer.\n",
        "\n",
        "Key idea: *If you canâ€™t gradient-check it, you donâ€™t really trust it.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 â€” Finite Differences (Gradient Checking)\n",
        "\n",
        "### Task 1.1: Implement numerical gradient for scalar f(w)\n",
        "\n",
        "# HINT:\n",
        "- Use central difference: (f(w+eps e_i) - f(w-eps e_i)) / (2 eps)\n",
        "\n",
        "**Explain:** Why is central difference more accurate than forward difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Central difference is more accurate than forward difference because its error term is second-order ð‘‚(â„Ž^2), while forward difference has only first-order error ð‘‚(â„Ž)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w :  [ 0.36159505  1.30400005  0.94708096 -0.70373524 -1.26542147]\n",
            "OK: grad_close\n"
          ]
        }
      ],
      "source": [
        "def numerical_grad(f, w, eps=1e-5):\n",
        "    # it takes input f, it takes 'w' (point at which i want Gradient)\n",
        "    # numerical gradient of f at w\n",
        "    w = np.asarray(w, dtype = float) # numpy array of floats\n",
        "    # print('w float : ', w)\n",
        "    g = np.zeros_like(w) # partial derivative of f -> same shape as w\n",
        "    # print('g : ', g)\n",
        "\n",
        "    for i in range(w.size):\n",
        "        e = np.zeros_like(w)\n",
        "        e[i] = 1.0 # for ith position, value is 0 -> basis vector\n",
        "        # print('e : ', e)\n",
        "        g[i] = (f(w + eps*e) - f(w - eps*e)) / (2 * eps) # Central difference: more effective than forward diff\n",
        "        # print('g[i] : ', g[i])\n",
        "    return g\n",
        "\n",
        "# sanity test: f(w)=sum(w^2) => grad=2w\n",
        "w = rng.standard_normal(5)\n",
        "print('w : ', w)\n",
        "f = lambda v: float(np.sum(v*v))\n",
        "g_num = numerical_grad(f, w)\n",
        "g_true = 2*w\n",
        "check('grad_close', np.allclose(g_num, g_true, atol=1e-6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 â€” Chain Rule in Code\n",
        "\n",
        "### Task 2.1: Gradient of MSE for linear model\n",
        "\n",
        "Model: y_hat = Xw\n",
        "Loss: L(w) = (1/n) * sum_i (y_hat_i - y_i)^2\n",
        "\n",
        "# HINT:\n",
        "- Let r = Xw - y\n",
        "- grad = (2/n) * X^T r\n",
        "\n",
        "**FAANG gotcha:** shape mismatches; keep w as (d,) and X as (n,d)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: mse_grad\n",
            "loss 7.244391404204653\n"
          ]
        }
      ],
      "source": [
        "def mse_loss_and_grad(X, y, w):\n",
        "    r = X @ w - y                   # gives residual prediction error vector using linear model\n",
        "    n = X.shape[0]                  # number of samples used for normalization\n",
        "    loss = float(np.mean(r * r))    # Compute mean squared error loss\n",
        "    grad = (2.0/n) * (X.T @ r)      # Apply grad chain rule\n",
        "    return loss, grad\n",
        "\n",
        "\n",
        "n, d = 20, 4\n",
        "X = rng.standard_normal((n, d))     # Generate input feature matrix\n",
        "w = rng.standard_normal(d)          # Initialize weight vector\n",
        "y = rng.standard_normal(n)          # Generate target values\n",
        "loss, grad = mse_loss_and_grad(X, y, w) # computing MSE loss and analytic gradient\n",
        "\n",
        "# gradient check\n",
        "f = lambda v: mse_loss_and_grad(X, y, v)[0]\n",
        "g_num = numerical_grad(f, w)\n",
        "check('mse_grad', np.allclose(grad, g_num, atol=1e-5))\n",
        "print('loss', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 â€” Logistic Regression (Core Interview Gradient)\n",
        "\n",
        "### Task 3.1: Binary cross-entropy gradient\n",
        "\n",
        "Given labels y in {0,1}.\n",
        "p = sigmoid(Xw)\n",
        "Loss = -(1/n) * sum(y log p + (1-y) log(1-p))\n",
        "\n",
        "# HINT:\n",
        "- sigmoid(z)=1/(1+exp(-z))\n",
        "- grad = (1/n) * X^T (p - y)\n",
        "- add numerical stability for logs (clip p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: logreg_grad\n",
            "loss 0.834822923472775\n"
          ]
        }
      ],
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))         # apply sigmoid to map real values to probabilities (0,1)\n",
        "\n",
        "def logreg_loss_and_grad(X, y, w):\n",
        "    z = X @ w       # linear part of our model\n",
        "    p = sigmoid(z)  # help make a prediction probability\n",
        "    eps = 1e-12     # binary probability\n",
        "    p_clip = np.clip(p, eps, 1.0 - eps)         # clip: to avoid log(0) during loss computation\n",
        "    loss = float(-np.mean(y * np.log(p_clip) + (1 - y) * np.log(1 - p_clip)))  # Compute binary cross-entropy (log loss)\n",
        "    grad = (X.T @ (p - y)) / X.shape[0]         # sum contributions from all features (Gradient via chain rule)\n",
        "    return loss, grad     \n",
        "\n",
        "\n",
        "n, d = 50, 3\n",
        "X = rng.standard_normal((n, d))\n",
        "w = rng.standard_normal(d)\n",
        "y = rng.integers(0, 2, size=n)\n",
        "loss, grad = logreg_loss_and_grad(X, y, w)      # compute logistic loss and analytic gradient\n",
        "\n",
        "f = lambda v: logreg_loss_and_grad(X, y, v)[0]  # define loss-only function for gradient checking\n",
        "g_num = numerical_grad(f, w, eps=1e-5)          # compute numerical gradient using finite differences\n",
        "\n",
        "check('logreg_grad', np.allclose(grad, g_num, atol=1e-5))\n",
        "print('loss', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 â€” Jacobian/Hessian Intuition\n",
        "\n",
        "### Task 4.1: Compute Hessian of f(w)=sum(w^2) numerically\n",
        "\n",
        "# HINT:\n",
        "- Hessian of sum(w^2) is 2I\n",
        "- Use numerical_grad on each component of grad\n",
        "\n",
        "This is mainly about *shape thinking*: Hessian is (d,d)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: H_shape\n",
            "OK: H_close\n",
            "[[ 1.99999999e+00 -2.22044605e-08  2.22044605e-08  0.00000000e+00]\n",
            " [-2.22044605e-08  1.99999999e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 2.22044605e-08  0.00000000e+00  1.99999999e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.99999999e+00]]\n"
          ]
        }
      ],
      "source": [
        "def numerical_hessian(f, w, eps=1e-4):\n",
        "    w = np.asarray(w, dtype=float)  # float numpy input array\n",
        "    d = w.size                      # number of parameters (dimension of input)\n",
        "    H = np.zeros((d, d))            # initializing empty Hessian matrix (dÃ—d)\n",
        "\n",
        "    for i in range(d):              # loop over each dimension to compute second derivatives\n",
        "        def fi(v):                  # define function representing partial derivative w.r.t ith variable\n",
        "            e = np.zeros(d); e[i] = 1.0     # create unit vector for i-th direction\n",
        "            return (f(v + eps * e) - f(v - eps*e)) / (2 * eps)  # compute central difference approximation of first derivative\n",
        "        \n",
        "        H[i, :] = numerical_grad(fi, w, eps = eps)  # compute gradient to obtain i-th row of Hessian (second derivatives)\n",
        "    return H                        # return full Hessian matrix\n",
        "\n",
        "w = rng.standard_normal(4)          # generate random parameter vector\n",
        "f = lambda v: float(np.sum(v*v))    # define quadratic scalar function\n",
        "H = numerical_hessian(f, w)         # Hessian of f at w\n",
        "\n",
        "check('H_shape', H.shape == (4,4))\n",
        "check('H_close', np.allclose(H, 2*np.eye(4), atol=1e-3))\n",
        "print(H)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Gradient checks pass\n",
        "- Explain prompts answered\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
