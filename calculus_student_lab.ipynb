{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": []},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {"cell_type":"markdown","metadata":{},"source":[
      "# Calculus & Gradients — FAANG-Level Lab\n",
      "\n",
      "**Goal:** Implement and verify gradients like an ML engineer.\n",
      "\n",
      "Key idea: *If you can’t gradient-check it, you don’t really trust it.*\n"
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "import numpy as np\n",
      "\n",
      "def check(name: str, cond: bool):\n",
      "    if not cond:\n",
      "        raise AssertionError(f'Failed: {name}')\n",
      "    print(f'OK: {name}')\n",
      "\n",
      "rng = np.random.default_rng(0)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 1 — Finite Differences (Gradient Checking)\n",
      "\n",
      "### Task 1.1: Implement numerical gradient for scalar f(w)\n",
      "\n",
      "# HINT:\n",
      "- Use central difference: (f(w+eps e_i) - f(w-eps e_i)) / (2 eps)\n",
      "\n",
      "**Explain:** Why is central difference more accurate than forward difference?"
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def numerical_grad(f, w, eps=1e-5):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "# sanity test: f(w)=sum(w^2) => grad=2w\n",
      "w = rng.standard_normal(5)\n",
      "f = lambda v: float(np.sum(v*v))\n",
      "g_num = numerical_grad(f, w)\n",
      "g_true = 2*w\n",
      "check('grad_close', np.allclose(g_num, g_true, atol=1e-6))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 2 — Chain Rule in Code\n",
      "\n",
      "### Task 2.1: Gradient of MSE for linear model\n",
      "\n",
      "Model: y_hat = Xw\n",
      "Loss: L(w) = (1/n) * sum_i (y_hat_i - y_i)^2\n",
      "\n",
      "# HINT:\n",
      "- Let r = Xw - y\n",
      "- grad = (2/n) * X^T r\n",
      "\n",
      "**FAANG gotcha:** shape mismatches; keep w as (d,) and X as (n,d)."
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def mse_loss_and_grad(X, y, w):\n",
      "    # TODO: return (loss, grad)\n",
      "    ...\n",
      "\n",
      "n, d = 20, 4\n",
      "X = rng.standard_normal((n, d))\n",
      "w = rng.standard_normal(d)\n",
      "y = rng.standard_normal(n)\n",
      "loss, grad = mse_loss_and_grad(X, y, w)\n",
      "\n",
      "# gradient check\n",
      "f = lambda v: mse_loss_and_grad(X, y, v)[0]\n",
      "g_num = numerical_grad(f, w)\n",
      "check('mse_grad', np.allclose(grad, g_num, atol=1e-5))\n",
      "print('loss', loss)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 3 — Logistic Regression (Core Interview Gradient)\n",
      "\n",
      "### Task 3.1: Binary cross-entropy gradient\n",
      "\n",
      "Given labels y in {0,1}.\n",
      "p = sigmoid(Xw)\n",
      "Loss = -(1/n) * sum(y log p + (1-y) log(1-p))\n",
      "\n",
      "# HINT:\n",
      "- sigmoid(z)=1/(1+exp(-z))\n",
      "- grad = (1/n) * X^T (p - y)\n",
      "- add numerical stability for logs (clip p)\n"
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def sigmoid(z):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "def logreg_loss_and_grad(X, y, w):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "n, d = 50, 3\n",
      "X = rng.standard_normal((n, d))\n",
      "w = rng.standard_normal(d)\n",
      "y = rng.integers(0, 2, size=n)\n",
      "loss, grad = logreg_loss_and_grad(X, y, w)\n",
      "f = lambda v: logreg_loss_and_grad(X, y, v)[0]\n",
      "g_num = numerical_grad(f, w, eps=1e-5)\n",
      "check('logreg_grad', np.allclose(grad, g_num, atol=1e-5))\n",
      "print('loss', loss)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 4 — Jacobian/Hessian Intuition\n",
      "\n",
      "### Task 4.1: Compute Hessian of f(w)=sum(w^2) numerically\n",
      "\n",
      "# HINT:\n",
      "- Hessian of sum(w^2) is 2I\n",
      "- Use numerical_grad on each component of grad\n",
      "\n",
      "This is mainly about *shape thinking*: Hessian is (d,d)."
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def numerical_hessian(f, w, eps=1e-4):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "w = rng.standard_normal(4)\n",
      "f = lambda v: float(np.sum(v*v))\n",
      "H = numerical_hessian(f, w)\n",
      "check('H_shape', H.shape == (4,4))\n",
      "check('H_close', np.allclose(H, 2*np.eye(4), atol=1e-3))\n",
      "print(H)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "---\n",
      "## Submission Checklist\n",
      "- All TODOs completed\n",
      "- Gradient checks pass\n",
      "- Explain prompts answered\n"
    ]}
  ]
}
